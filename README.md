# MBG-Activation-Functions
<i>Mario Banuelos Research Group: Activation Functions Project</i>

<b>Abstract</b>
Artificial neural networks are computer models that “learn” to perform certain tasks (such as image classification) by processing prodigious amounts of labelled input data. Like the neural networks in our brain, these artificial neural networks learn how to perform a task by reinforcing pathways that lead to success and abandoning pathways that lead to failure. Activation functions play a key role in neural networks by introducing non-linearity into the system, which is needed to help the model accommodate a wide variety of potential classification schemes. However, activation functions are often overlooked by researchers in the field of deep learning, who prefer to rely on standard ones like ReLU and Tanh without considering whether they are the best for a given task. We investigate two new activation functions, called TAct and mTAct, which interpolate between other activation functions, depending on the task. We explore the role of these functions on accuracy over a multitude of neural network architectures varying in complexity and learning rate. These activation functions were tested in the context of image classification, using an extensive image dataset known as CIFAR-10. Preliminary results show that TAct and mTAct seem to perform better than standard activation functions on simple models with few parameters and low learning rates. Although the mathematics behind the interactions of activation functions within the model is still not fully understood, the behavior of TAct and mTAct provides some insight into this unresolved matter.
