epoch     train_loss  valid_loss  accuracy  time    
0         10.242156   #na#        00:03     
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
Min numerical gradient: 6.92E-04
Min loss divided by 10: 7.59E-04
epoch     train_loss  valid_loss  accuracy  time    
/home/gobind/miniconda3/lib/python3.7/site-packages/fastai/sixel.py:16: UserWarning: You could see this plot with `libsixel`. See https://github.com/saitoha/libsixel warn("You could see this plot with `libsixel`. See https://github.com/saitoha/libsixel")
0         1.389844    1.385650    0.504200  00:19     
1         1.200340    1.297001    0.542000  00:19     
2         1.052657    1.153969    0.591500  00:19     
3         0.890454    1.107243    0.621000  00:19     
4         0.774275    1.072927    0.641000  00:19     
5         0.638182    1.152464    0.628600  00:19     
6         0.531018    1.262745    0.622200  00:19     
7         0.420087    1.369955    0.623800  00:19     
8         0.355435    1.543695    0.621500  00:19     
9         0.288602    1.575057    0.621100  00:19     
10        0.243888    1.677646    0.619000  00:12     
11        0.217329    1.768145    0.626700  00:18     
12        0.204453    1.916439    0.613600  00:18     
13        0.183447    1.814434    0.618300  00:18     
14        0.149583    1.876426    0.622400  00:18     
15        0.146805    1.999446    0.623800  00:18     
16        0.138162    2.039182    0.623700  00:18     
17        0.128484    2.026924    0.629000  00:18     
18        0.120330    2.025335    0.634900  00:18     
19        0.125203    2.123933    0.609700  00:18     
20        0.108731    2.065565    0.627100  00:18     
21        0.102633    2.084594    0.627000  00:18     
22        0.099682    2.180670    0.620500  00:18     
23        0.107707    2.174203    0.623500  00:18     
24        0.089484    2.104339    0.633400  00:18     
25        0.089503    2.331504    0.620300  00:18     
26        0.089343    2.150940    0.626700  00:18     
27        0.070783    2.262216    0.631100  00:18     
28        0.083327    2.380253    0.629400  00:18     
29        0.075570    2.261789    0.622400  00:18     
30        0.072857    2.248429    0.632400  00:18     
31        0.078340    2.339286    0.629700  00:18     
32        0.068571    2.372850    0.635000  00:18     
33        0.069719    2.422106    0.631800  00:18     
34        0.066091    2.393950    0.627600  00:18     
35        0.055344    2.370248    0.638200  00:18     
36        0.062273    2.353503    0.629900  00:18     
37        0.059564    2.458028    0.630400  00:18     
38        0.060526    2.369489    0.633200  00:18     
39        0.052472    2.432801    0.628200  00:18     
40        0.059695    2.419806    0.622600  00:18     
41        0.054635    2.464445    0.629400  00:18     
42        0.052329    2.470942    0.635800  00:18     
43        0.059879    2.481019    0.633600  00:18     
44        0.045731    2.488769    0.628400  00:18     
45        0.048327    2.466897    0.630700  00:18     
46        0.044300    2.485648    0.631700  00:18     
47        0.051707    2.683665    0.629600  00:18     
48        0.045824    2.456778    0.631100  00:18     
49        0.045160    2.542043    0.633900  00:18     
50        0.038179    2.498538    0.632600  00:18     
51        0.043013    2.557244    0.630200  00:18     
52        0.043062    2.638596    0.635400  00:18     
53        0.038098    2.670815    0.623900  00:18     
54        0.044868    2.701225    0.632100  00:18     
55        0.046442    2.609282    0.633200  00:18     
56        0.036660    2.538827    0.643500  00:18     
57        0.033459    2.587632    0.639800  00:18     
58        0.047675    2.491103    0.637700  00:18     
59        0.041354    2.581420    0.636000  00:18     
60        0.029141    2.582069    0.641500  00:18     
61        0.036583    2.590167    0.636900  00:18     
62        0.035840    2.599264    0.635700  00:18     
63        0.035122    2.467469    0.643700  00:18     
64        0.036615    2.592010    0.635400  00:18     
65        0.037695    2.692806    0.635900  00:18     
66        0.033347    2.696340    0.637400  00:18     
67        0.032270    2.720128    0.634000  00:18     
68        0.029016    2.694840    0.638900  00:18     
69        0.036399    2.608339    0.642000  00:18     
70        0.034637    2.818726    0.635000  00:18     
71        0.032214    2.717269    0.633200  00:18     
72        0.034385    2.703537    0.635900  00:18     
73        0.030424    2.762677    0.634000  00:18     
74        0.032249    2.752316    0.632900  00:18     
75        0.029507    2.700011    0.642400  00:18     
76        0.026461    2.746310    0.643400  00:18     
77        0.030357    2.807043    0.631000  00:18     
78        0.037659    2.732949    0.639300  00:18     
79        0.029812    2.783110    0.642100  00:18     
80        0.031510    2.745159    0.636600  00:18     
81        0.026138    2.793151    0.639100  00:18     
82        0.030784    2.800707    0.630600  00:18     
83        0.030867    2.750735    0.642700  00:18     
84        0.024639    2.767744    0.640300  00:18     
85        0.025166    2.800360    0.635800  00:18     
86        0.026024    2.834027    0.640200  00:18     
87        0.030634    2.866519    0.640500  00:18     
88        0.030054    2.842166    0.639100  00:18     
89        0.031407    2.868934    0.642400  00:18     
90        0.028563    2.890615    0.634600  00:18     
91        0.030951    2.713983    0.634600  00:18     
92        0.023878    2.869283    0.632300  00:18     
93        0.024092    2.758434    0.645000  00:18     
94        0.022593    2.790778    0.639800  00:18     
95        0.029551    2.843490    0.634800  00:18     
96        0.024496    2.787369    0.642200  00:18     
97        0.019578    2.860338    0.637100  00:18     
98        0.018369    2.837050    0.640600  00:18     
99        0.026334    2.859804    0.637200  00:18     
