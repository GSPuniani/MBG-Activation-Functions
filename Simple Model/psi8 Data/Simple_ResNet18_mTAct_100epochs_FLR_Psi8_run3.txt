epoch     train_loss  valid_loss  accuracy  time    
0         10.978869   #na#        00:02     
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
Min numerical gradient: 2.29E-04
Min loss divided by 10: 6.31E-04
epoch     train_loss  valid_loss  accuracy  time    
/home/gobind/miniconda3/lib/python3.7/site-packages/fastai/sixel.py:16: UserWarning: You could see this plot with `libsixel`. See https://github.com/saitoha/libsixel warn("You could see this plot with `libsixel`. See https://github.com/saitoha/libsixel")
0         1.510330    1.461073    0.475300  00:09     
1         1.307433    1.337633    0.516400  00:09     
2         1.118529    1.310810    0.531800  00:09     
3         0.941400    1.405766    0.525500  00:09     
4         0.735021    1.430919    0.546700  00:09     
5         0.537738    1.657395    0.530800  00:09     
6         0.410706    1.788608    0.536100  00:09     
7         0.323754    1.926460    0.539600  00:09     
8         0.273619    2.112846    0.535100  00:09     
9         0.226430    2.215390    0.535100  00:09     
10        0.209756    2.281311    0.541300  00:09     
11        0.184132    2.337321    0.531800  00:10     
12        0.176685    2.386179    0.529200  00:10     
13        0.160768    2.483007    0.542800  00:09     
14        0.144832    2.438768    0.545300  00:10     
15        0.161501    2.474048    0.532200  00:10     
16        0.136580    2.581140    0.538500  00:10     
17        0.127121    2.508283    0.540700  00:10     
18        0.123103    2.629721    0.531500  00:10     
19        0.118280    2.666907    0.539200  00:10     
20        0.115296    2.572433    0.542300  00:09     
21        0.117449    2.629017    0.544300  00:10     
22        0.105156    2.697880    0.541000  00:10     
23        0.109893    2.678695    0.539700  00:10     
24        0.089908    2.664595    0.552500  00:10     
25        0.097244    2.720349    0.543800  00:10     
26        0.093469    2.709624    0.538400  00:10     
27        0.086012    2.702821    0.549600  00:09     
28        0.083068    2.796108    0.545900  00:10     
29        0.076109    2.800806    0.552400  00:10     
30        0.095220    2.793835    0.547500  00:10     
31        0.077541    2.822931    0.545500  00:10     
32        0.078418    2.811281    0.544200  00:10     
33        0.081554    2.874350    0.549100  00:10     
34        0.080560    2.934249    0.540900  00:10     
35        0.066495    2.896880    0.551200  00:10     
36        0.079745    2.883187    0.552500  00:10     
37        0.074469    2.808317    0.554500  00:10     
38        0.060706    2.929763    0.553900  00:10     
39        0.064792    2.905996    0.550300  00:10     
40        0.065789    3.014919    0.552400  00:10     
41        0.062226    2.945784    0.550500  00:10     
42        0.056434    2.876694    0.552500  00:10     
43        0.060809    2.887469    0.551600  00:10     
44        0.065292    3.004133    0.544400  00:10     
45        0.052296    2.855180    0.559700  00:10     
46        0.056831    2.955073    0.559200  00:10     
47        0.052045    2.888150    0.557900  00:10     
48        0.046714    2.995307    0.555500  00:10     
49        0.049604    3.027595    0.547700  00:10     
50        0.050247    3.053844    0.558800  00:10     
51        0.053738    2.979515    0.554600  00:10     
52        0.052651    3.013258    0.552500  00:09     
53        0.044439    3.106618    0.558000  00:10     
54        0.049638    3.106445    0.548500  00:09     
55        0.047169    3.095040    0.560100  00:10     
56        0.041989    2.987181    0.560400  00:10     
57        0.052994    3.240486    0.552100  00:10     
58        0.046229    3.154300    0.554000  00:10     
59        0.049170    3.011502    0.564700  00:10     
60        0.054485    3.051085    0.558700  00:10     
61        0.047150    3.054154    0.565300  00:10     
62        0.045394    3.046919    0.553100  00:10     
63        0.040087    3.060012    0.559500  00:10     
64        0.041454    3.072354    0.560500  00:10     
65        0.037784    3.174750    0.556600  00:10     
66        0.040639    3.061094    0.565600  00:10     
67        0.038093    3.125268    0.556400  00:10     
68        0.037055    3.031968    0.565400  00:10     
69        0.043221    3.085175    0.562600  00:10     
70        0.041144    3.073426    0.562800  00:10     
71        0.042508    3.180101    0.557400  00:10     
72        0.039582    3.099758    0.559400  00:10     
73        0.040061    3.126625    0.569100  00:10     
74        0.040618    3.128033    0.561000  00:10     
75        0.038751    3.155093    0.556000  00:10     
76        0.035124    3.158629    0.568400  00:10     
77        0.031531    3.143047    0.567400  00:10     
78        0.032066    3.270602    0.560700  00:10     
79        0.033579    3.275747    0.560600  00:10     
80        0.037403    3.270891    0.562600  00:10     
81        0.037863    3.201680    0.564500  00:10     
82        0.033590    3.201722    0.565100  00:10     
83        0.035767    3.313560    0.567800  00:10     
84        0.031912    3.270571    0.575100  00:10     
85        0.036630    3.253509    0.565900  00:10     
86        0.038394    3.185620    0.566400  00:10     
87        0.030273    3.169921    0.569700  00:10     
88        0.032832    3.258293    0.555500  00:10     
89        0.045927    3.281296    0.559600  00:10     
90        0.030274    3.271796    0.566900  00:10     
91        0.027905    3.272147    0.561700  00:10     
92        0.028899    3.196311    0.568200  00:10     
93        0.034920    3.227490    0.565200  00:10     
94        0.031038    3.312967    0.566000  00:10     
95        0.032657    3.244131    0.564600  00:10     
96        0.033222    3.264510    0.566800  00:09     
97        0.031681    3.377340    0.558800  00:10     
98        0.033701    3.312824    0.568500  00:10     
99        0.031057    3.322284    0.566500  00:10     
