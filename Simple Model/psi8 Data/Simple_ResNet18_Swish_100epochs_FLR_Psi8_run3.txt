epoch     train_loss  valid_loss  accuracy  time    
0         10.958284   #na#        00:03     
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
Min numerical gradient: 1.20E-03
Min loss divided by 10: 7.59E-04
epoch     train_loss  valid_loss  accuracy  time    
/home/gobind/miniconda3/lib/python3.7/site-packages/fastai/sixel.py:16: UserWarning: You could see this plot with `libsixel`. See https://github.com/saitoha/libsixel warn("You could see this plot with `libsixel`. See https://github.com/saitoha/libsixel")
0         1.399576    1.430164    0.489700  00:18     
1         1.205709    1.240481    0.556900  00:18     
2         1.039228    1.135823    0.600900  00:18     
3         0.922353    1.114062    0.611900  00:18     
4         0.801953    1.122651    0.628500  00:18     
5         0.683769    1.077325    0.644000  00:18     
6         0.567592    1.247196    0.623200  00:18     
7         0.469984    1.227424    0.639500  00:18     
8         0.385057    1.317139    0.640700  00:18     
9         0.298352    1.481244    0.645300  00:18     
10        0.245673    1.607783    0.637700  00:13     
11        0.216957    1.674990    0.635500  00:18     
12        0.202344    1.676727    0.633100  00:18     
13        0.179764    1.773483    0.638900  00:18     
14        0.155416    1.833087    0.634700  00:18     
15        0.134818    1.927883    0.642700  00:18     
16        0.128105    1.937087    0.643300  00:18     
17        0.123883    1.954803    0.641600  00:18     
18        0.115744    2.004112    0.646200  00:18     
19        0.113630    1.964882    0.637100  00:18     
20        0.105441    2.019094    0.646000  00:18     
21        0.100038    2.028250    0.646800  00:18     
22        0.091907    2.077105    0.640000  00:18     
23        0.088640    2.076808    0.642400  00:18     
24        0.083183    2.007831    0.646000  00:18     
25        0.074597    2.149138    0.643600  00:18     
26        0.087187    2.142668    0.644700  00:18     
27        0.071706    2.166357    0.641300  00:18     
28        0.076387    2.189894    0.646200  00:18     
29        0.075426    2.186045    0.641400  00:18     
30        0.070450    2.271382    0.639500  00:18     
31        0.070113    2.226416    0.649600  00:18     
32        0.065060    2.277646    0.642200  00:18     
33        0.062137    2.281920    0.640600  00:18     
34        0.059298    2.172272    0.650900  00:18     
35        0.065222    2.231954    0.640500  00:18     
36        0.056535    2.284255    0.645300  00:18     
37        0.066182    2.275394    0.649200  00:18     
38        0.054141    2.458301    0.642900  00:18     
39        0.055568    2.463745    0.642700  00:18     
40        0.055216    2.272980    0.643900  00:18     
41        0.047174    2.342571    0.642600  00:18     
42        0.049953    2.443439    0.640900  00:18     
43        0.054742    2.412165    0.646000  00:18     
44        0.059891    2.380727    0.637900  00:18     
45        0.044829    2.389311    0.647700  00:18     
46        0.052313    2.364670    0.649700  00:18     
47        0.042940    2.498127    0.642600  00:18     
48        0.045261    2.492550    0.642300  00:18     
49        0.041301    2.463802    0.641900  00:18     
50        0.040442    2.318071    0.648400  00:18     
51        0.042425    2.421484    0.641700  00:18     
52        0.041412    2.550512    0.646200  00:18     
53        0.036486    2.449561    0.642500  00:18     
54        0.045509    2.556230    0.648100  00:18     
55        0.038226    2.437020    0.645900  00:18     
56        0.037753    2.472043    0.646100  00:18     
57        0.040012    2.429296    0.645900  00:18     
58        0.037442    2.541573    0.640500  00:18     
59        0.040809    2.508163    0.640100  00:18     
60        0.042823    2.523139    0.643600  00:18     
61        0.028663    2.486477    0.647000  00:18     
62        0.042125    2.654285    0.645300  00:18     
63        0.038563    2.541018    0.644200  00:18     
64        0.030535    2.497526    0.649300  00:18     
65        0.033509    2.402112    0.652200  00:18     
66        0.029912    2.671963    0.646400  00:18     
67        0.037546    2.592713    0.646400  00:18     
68        0.032135    2.605768    0.644900  00:18     
69        0.039968    2.514181    0.643800  00:18     
70        0.030833    2.628839    0.645200  00:18     
71        0.033224    2.664120    0.642100  00:18     
72        0.032881    2.580392    0.652000  00:18     
73        0.030257    2.563339    0.647800  00:18     
74        0.037920    2.617929    0.648900  00:18     
75        0.034528    2.665164    0.646400  00:18     
76        0.029295    2.641769    0.649700  00:18     
77        0.028681    2.648562    0.643400  00:18     
78        0.030005    2.707783    0.645000  00:18     
79        0.032271    2.535592    0.649000  00:18     
80        0.026279    2.870234    0.638900  00:18     
81        0.031192    2.669431    0.641400  00:18     
82        0.032092    2.654473    0.650200  00:18     
83        0.025630    2.757411    0.645400  00:18     
84        0.021691    2.722951    0.646500  00:18     
85        0.029359    2.644733    0.645400  00:18     
86        0.026826    2.640635    0.645900  00:18     
87        0.029217    2.620014    0.651000  00:18     
88        0.029527    2.595558    0.649200  00:18     
89        0.025842    2.645977    0.649700  00:18     
90        0.028834    2.647280    0.644000  00:18     
91        0.025795    2.588581    0.652300  00:18     
92        0.027883    2.743984    0.647900  00:18     
93        0.027361    2.538284    0.650500  00:18     
94        0.023892    2.547922    0.648100  00:18     
95        0.035843    2.731510    0.645300  00:18     
96        0.031847    2.601339    0.652800  00:18     
97        0.022710    2.667459    0.649600  00:18     
98        0.023269    2.563845    0.649400  00:18     
99        0.030508    2.856055    0.635700  00:18     
