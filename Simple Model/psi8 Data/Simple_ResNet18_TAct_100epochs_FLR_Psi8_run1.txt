epoch     train_loss  valid_loss  accuracy  time    
0         11.946676   #na#        00:07     
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
Min numerical gradient: 5.75E-04
Min loss divided by 10: 1.58E-03
epoch     train_loss  valid_loss  accuracy  time    
/home/gobind/miniconda3/lib/python3.7/site-packages/fastai/sixel.py:16: UserWarning: You could see this plot with `libsixel`. See https://github.com/saitoha/libsixel warn("You could see this plot with `libsixel`. See https://github.com/saitoha/libsixel")
0         1.411625    1.470387    0.476500  00:28     
1         1.214641    1.311361    0.543800  00:22     
2         1.024669    1.123333    0.604500  00:17     
3         0.889929    1.096548    0.616600  00:18     
4         0.769088    1.142772    0.618600  00:17     
5         0.633390    1.257272    0.609800  00:18     
6         0.498768    1.252981    0.631200  00:18     
7         0.393046    1.366503    0.622900  00:16     
8         0.324618    1.520554    0.619400  00:15     
9         0.261340    1.689335    0.615500  00:18     
10        0.233566    1.723846    0.613900  00:18     
11        0.206606    1.789244    0.610800  00:17     
12        0.186509    1.810499    0.615300  00:17     
13        0.169105    1.918593    0.616700  00:17     
14        0.165744    1.913524    0.617600  00:17     
15        0.146830    1.992596    0.611900  00:17     
16        0.144890    1.977037    0.622600  00:18     
17        0.132701    2.006507    0.618300  00:17     
18        0.129148    2.045193    0.621800  00:18     
19        0.112572    2.143954    0.621700  00:18     
20        0.117786    2.114996    0.619700  00:17     
21        0.107921    2.159336    0.620800  00:17     
22        0.099405    2.192898    0.617200  00:17     
23        0.093666    2.105127    0.633100  00:17     
24        0.094804    2.168232    0.623700  00:17     
25        0.084577    2.299023    0.626200  00:18     
26        0.077968    2.301887    0.613100  00:17     
27        0.088779    2.291313    0.619700  00:17     
28        0.080347    2.284241    0.625900  00:17     
29        0.073947    2.425828    0.619600  00:17     
30        0.080066    2.357323    0.627100  00:17     
31        0.079854    2.308418    0.622800  00:18     
32        0.075651    2.376538    0.619200  00:17     
33        0.063020    2.310847    0.628000  00:17     
34        0.075398    2.368489    0.624000  00:17     
35        0.066626    2.382713    0.627000  00:18     
36        0.065592    2.387221    0.623700  00:18     
37        0.066632    2.607756    0.611000  00:17     
38        0.053415    2.413783    0.625900  00:17     
39        0.057482    2.502641    0.624200  00:17     
40        0.057540    2.564527    0.620100  00:18     
41        0.057610    2.564941    0.625800  00:17     
42        0.055580    2.492077    0.627100  00:17     
43        0.050666    2.485594    0.619900  00:18     
44        0.053584    2.582093    0.622900  00:18     
45        0.048501    2.511057    0.625700  00:18     
46        0.056742    2.511785    0.627300  00:17     
47        0.048681    2.550251    0.629400  00:17     
48        0.047542    2.518841    0.632800  00:18     
49        0.047363    2.546848    0.627700  00:17     
50        0.043313    2.524095    0.626400  00:18     
51        0.047069    2.650151    0.626000  00:17     
52        0.048624    2.642009    0.626400  00:18     
53        0.044402    2.549680    0.630400  00:18     
54        0.046915    2.648058    0.632500  00:18     
55        0.040634    2.761978    0.624400  00:17     
56        0.037183    2.577644    0.631300  00:18     
57        0.041327    2.607828    0.633600  00:18     
58        0.040160    2.579795    0.633100  00:17     
59        0.032816    2.689898    0.626800  00:17     
60        0.034391    2.658514    0.633300  00:17     
61        0.034202    2.694817    0.627200  00:17     
62        0.034305    2.854631    0.629900  00:17     
63        0.032909    2.685905    0.633200  00:17     
64        0.036219    2.688026    0.635400  00:17     
65        0.032993    2.626745    0.635500  00:17     
66        0.037820    2.710656    0.635500  00:17     
67        0.034536    2.696637    0.632400  00:17     
68        0.034895    2.815455    0.628500  00:17     
69        0.038021    2.705875    0.624700  00:17     
70        0.032021    2.626724    0.630000  00:17     
71        0.029730    2.762265    0.629500  00:18     
72        0.034753    2.747081    0.636000  00:18     
73        0.032849    2.635241    0.639400  00:17     
74        0.031471    2.765558    0.630200  00:17     
75        0.032151    2.846514    0.630700  00:18     
76        0.026431    2.758013    0.635100  00:17     
77        0.030531    2.797269    0.630500  00:18     
78        0.037233    2.771196    0.635100  00:18     
79        0.033046    2.807367    0.633700  00:17     
80        0.027719    2.805205    0.630800  00:17     
81        0.034169    2.850769    0.634200  00:17     
82        0.027990    2.807461    0.638000  00:18     
83        0.028472    2.826276    0.631900  00:17     
84        0.038832    2.887520    0.625700  00:18     
85        0.029718    2.796261    0.632100  00:17     
86        0.030138    2.755900    0.632200  00:17     
87        0.024548    2.851049    0.631000  00:17     
88        0.026031    2.887399    0.629900  00:18     
89        0.022000    2.827802    0.637500  00:18     
90        0.022144    2.769606    0.633700  00:17     
91        0.020341    2.876454    0.634700  00:18     
92        0.027413    2.894658    0.628900  00:17     
93        0.024550    2.861422    0.628400  00:17     
94        0.026590    2.931315    0.626800  00:17     
95        0.025939    3.036433    0.627500  00:17     
96        0.022717    2.975553    0.634800  00:17     
97        0.022535    2.946667    0.632700  00:18     
98        0.028917    2.859823    0.635200  00:18     
99        0.025634    2.919449    0.635900  00:17     
