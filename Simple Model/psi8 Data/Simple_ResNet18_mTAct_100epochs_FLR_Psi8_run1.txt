epoch     train_loss  valid_loss  accuracy  time    
0         13.459911   #na#        00:05     
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
Min numerical gradient: 1.20E-03
Min loss divided by 10: 3.63E-04
epoch     train_loss  valid_loss  accuracy  time    
/home/gobind/miniconda3/lib/python3.7/site-packages/fastai/sixel.py:16: UserWarning: You could see this plot with `libsixel`. See https://github.com/saitoha/libsixel warn("You could see this plot with `libsixel`. See https://github.com/saitoha/libsixel")
0         1.375836    1.363854    0.512500  00:18     
1         1.188895    1.338229    0.537200  00:17     
2         1.019286    1.189883    0.587000  00:18     
3         0.903937    1.087699    0.624900  00:18     
4         0.790028    1.126200    0.619500  00:18     
5         0.668839    1.126165    0.633900  00:17     
6         0.574243    1.141850    0.640700  00:17     
7         0.451570    1.279629    0.638600  00:12     
8         0.357518    1.393393    0.639000  00:18     
9         0.319468    1.425697    0.643400  00:17     
10        0.272917    1.547390    0.630500  00:17     
11        0.225453    1.617571    0.640000  00:18     
12        0.199831    1.765580    0.630300  00:18     
13        0.183311    1.744254    0.639200  00:18     
14        0.157526    1.790720    0.636500  00:18     
15        0.155377    1.795768    0.638300  00:18     
16        0.145995    1.949174    0.632600  00:18     
17        0.125472    1.996586    0.642700  00:18     
18        0.106597    2.005145    0.642000  00:18     
19        0.102698    1.984297    0.644500  00:18     
20        0.107678    1.953299    0.641800  00:18     
21        0.090660    2.109699    0.642400  00:18     
22        0.087838    2.111306    0.636000  00:18     
23        0.087005    2.043617    0.641700  00:18     
24        0.082857    2.201027    0.631700  00:18     
25        0.082209    2.126079    0.646400  00:18     
26        0.089823    2.059487    0.643500  00:18     
27        0.077039    2.175266    0.634300  00:18     
28        0.078963    2.212194    0.637900  00:18     
29        0.073064    2.213033    0.641400  00:18     
30        0.076038    2.235616    0.638800  00:18     
31        0.062452    2.296874    0.638000  00:18     
32        0.059766    2.212085    0.640300  00:18     
33        0.063767    2.315644    0.643600  00:18     
34        0.066408    2.258488    0.636300  00:18     
35        0.066226    2.299754    0.642700  00:18     
36        0.054415    2.341760    0.639900  00:18     
37        0.058006    2.275509    0.647300  00:18     
38        0.047265    2.338459    0.641800  00:18     
39        0.054204    2.313707    0.638200  00:18     
40        0.062709    2.490776    0.632300  00:18     
41        0.050253    2.343260    0.641100  00:18     
42        0.052481    2.343681    0.646800  00:18     
43        0.051819    2.368663    0.639700  00:18     
44        0.043869    2.365229    0.639900  00:18     
45        0.054472    2.389091    0.641300  00:18     
46        0.048912    2.479655    0.637700  00:18     
47        0.042334    2.419652    0.640800  00:18     
48        0.045145    2.447417    0.645300  00:18     
49        0.046464    2.556673    0.634600  00:18     
50        0.033930    2.526160    0.642500  00:18     
51        0.038835    2.562521    0.640600  00:18     
52        0.048094    2.508389    0.642700  00:18     
53        0.047339    2.531957    0.642300  00:18     
54        0.038035    2.451639    0.638700  00:18     
55        0.030985    2.583068    0.640900  00:18     
56        0.047518    2.489818    0.646700  00:18     
57        0.043320    2.510495    0.646500  00:18     
58        0.040849    2.550465    0.646700  00:18     
59        0.037399    2.641822    0.645900  00:18     
60        0.039200    2.584695    0.638600  00:18     
61        0.040729    2.723086    0.634300  00:18     
62        0.037707    2.529883    0.643400  00:18     
63        0.033609    2.551167    0.645600  00:18     
64        0.027741    2.638552    0.643600  00:18     
65        0.035608    2.432088    0.651100  00:18     
66        0.033553    2.594864    0.643800  00:18     
67        0.028200    2.726648    0.639200  00:18     
68        0.031069    2.515178    0.646600  00:18     
69        0.039610    2.651677    0.635200  00:18     
70        0.031382    2.637360    0.640700  00:18     
71        0.031388    2.607004    0.645000  00:18     
72        0.032530    2.614064    0.637300  00:18     
73        0.032959    2.541175    0.644900  00:18     
74        0.028138    2.664275    0.646700  00:18     
75        0.032638    2.573320    0.643600  00:18     
76        0.031726    2.646590    0.644100  00:18     
77        0.031341    2.571055    0.648900  00:18     
78        0.034553    2.600798    0.639700  00:18     
79        0.034748    2.600646    0.644900  00:18     
80        0.025300    2.553642    0.644700  00:18     
81        0.031524    2.644454    0.649800  00:18     
82        0.034895    2.623222    0.646300  00:18     
83        0.026294    2.649894    0.646500  00:18     
84        0.027691    2.719267    0.645400  00:18     
85        0.029640    2.520624    0.650900  00:18     
86        0.028732    2.584845    0.646400  00:18     
87        0.024918    2.743450    0.637500  00:18     
88        0.032944    2.696817    0.647200  00:18     
89        0.027531    2.647126    0.647300  00:18     
90        0.026628    2.717976    0.647000  00:18     
91        0.030777    2.652623    0.647700  00:18     
92        0.026099    2.652265    0.646000  00:18     
93        0.027535    2.615181    0.648500  00:18     
94        0.025270    2.811913    0.645200  00:18     
95        0.027737    2.672339    0.646500  00:18     
96        0.026339    2.666840    0.649400  00:18     
97        0.024264    2.657758    0.646600  00:18     
98        0.030354    2.708068    0.643000  00:18     
99        0.026885    2.711725    0.641300  00:18     
