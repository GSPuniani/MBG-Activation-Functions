epoch     train_loss  valid_loss  accuracy  time    
0         15.919189   #na#        00:04     
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
Min numerical gradient: 5.75E-04
Min loss divided by 10: 4.37E-04
epoch     train_loss  valid_loss  accuracy  time    
/home/gobind/miniconda3/lib/python3.7/site-packages/fastai/sixel.py:16: UserWarning: You could see this plot with `libsixel`. See https://github.com/saitoha/libsixel warn("You could see this plot with `libsixel`. See https://github.com/saitoha/libsixel")
0         1.416748    1.395641    0.497700  00:19     
1         1.209622    1.275545    0.554800  00:19     
2         1.064460    1.125470    0.597000  00:18     
3         0.884740    1.139625    0.606000  00:18     
4         0.769418    1.145528    0.619900  00:18     
5         0.613491    1.232844    0.616200  00:18     
6         0.507437    1.278166    0.618900  00:18     
7         0.405352    1.403940    0.612200  00:18     
8         0.325230    1.584347    0.612200  00:12     
9         0.255992    1.725951    0.600000  00:19     
10        0.224525    1.744751    0.608900  00:19     
11        0.221466    1.772027    0.613200  00:19     
12        0.181654    1.887890    0.612200  00:19     
13        0.163373    2.005821    0.605100  00:19     
14        0.154148    1.941525    0.617300  00:19     
15        0.148479    2.030358    0.612600  00:19     
16        0.145158    2.041357    0.613000  00:19     
17        0.132508    2.033007    0.614800  00:19     
18        0.124595    2.143207    0.618800  00:19     
19        0.111563    2.058964    0.613400  00:19     
20        0.100958    2.091463    0.627800  00:19     
21        0.107974    2.196035    0.617000  00:19     
22        0.099909    2.159711    0.613800  00:19     
23        0.104726    2.240997    0.617800  00:18     
24        0.089146    2.218117    0.622300  00:19     
25        0.099488    2.236004    0.621000  00:19     
26        0.089572    2.307675    0.615800  00:19     
27        0.078299    2.303289    0.620700  00:19     
28        0.084344    2.264197    0.626100  00:19     
29        0.074430    2.328596    0.619200  00:19     
30        0.081099    2.372599    0.615200  00:19     
31        0.079528    2.356986    0.616300  00:19     
32        0.081266    2.364566    0.621300  00:19     
33        0.070988    2.443089    0.615000  00:18     
34        0.058629    2.420281    0.621900  00:19     
35        0.079370    2.360540    0.619700  00:19     
36        0.056075    2.380252    0.622100  00:19     
37        0.058642    2.335886    0.623000  00:18     
38        0.072368    2.427200    0.626700  00:19     
39        0.049355    2.638499    0.613400  00:19     
40        0.055688    2.446973    0.627200  00:19     
41        0.048049    2.504109    0.628300  00:19     
42        0.048870    2.536002    0.623600  00:18     
43        0.045015    2.464984    0.618600  00:18     
44        0.058027    2.522619    0.622500  00:19     
45        0.046922    2.503600    0.622000  00:19     
46        0.051268    2.594059    0.619400  00:19     
47        0.046059    2.531441    0.623200  00:19     
48        0.049144    2.580526    0.619200  00:19     
49        0.049334    2.536717    0.621200  00:19     
50        0.044201    2.566928    0.618900  00:19     
51        0.036424    2.584832    0.620300  00:19     
52        0.042735    2.577188    0.630800  00:19     
53        0.042154    2.605052    0.623300  00:19     
54        0.038646    2.697666    0.625300  00:19     
55        0.049135    2.669466    0.626700  00:19     
56        0.043034    2.567398    0.625900  00:18     
57        0.039600    2.663620    0.633600  00:19     
58        0.034216    2.699950    0.624600  00:19     
59        0.040722    2.591916    0.631700  00:19     
60        0.035674    2.686482    0.627900  00:19     
61        0.041261    2.661850    0.629700  00:18     
62        0.032263    2.565884    0.631000  00:19     
63        0.037481    2.714201    0.625900  00:19     
64        0.037809    2.583389    0.632000  00:19     
65        0.035366    2.733485    0.626800  00:19     
66        0.034522    2.663695    0.630000  00:19     
67        0.039029    2.698963    0.627900  00:19     
68        0.035305    2.715951    0.622400  00:18     
69        0.035007    2.676358    0.625300  00:18     
70        0.035151    2.794275    0.628000  00:19     
71        0.028740    2.766371    0.628500  00:19     
72        0.029055    2.830153    0.630200  00:19     
73        0.030289    2.852017    0.628500  00:19     
74        0.029783    2.772598    0.630300  00:18     
75        0.031896    2.860880    0.632500  00:19     
76        0.033642    2.822123    0.625600  00:19     
77        0.034314    2.789676    0.626900  00:19     
78        0.031696    2.909538    0.627500  00:19     
79        0.028110    2.858737    0.624500  00:19     
80        0.026047    2.784927    0.627300  00:18     
81        0.029805    2.938384    0.626200  00:18     
82        0.025645    2.919836    0.631700  00:19     
83        0.025541    2.771253    0.630800  00:19     
84        0.023582    2.848657    0.633700  00:19     
85        0.028136    2.787322    0.634000  00:18     
86        0.030362    2.915293    0.626700  00:19     
87        0.030880    2.878448    0.628400  00:19     
88        0.032241    2.850896    0.627800  00:19     
89        0.027319    2.847857    0.630000  00:19     
90        0.024459    2.919846    0.624800  00:19     
91        0.022242    2.839720    0.634500  00:19     
92        0.029878    2.937140    0.632700  00:19     
93        0.024930    2.895302    0.630800  00:19     
94        0.025898    2.869497    0.629100  00:19     
95        0.024769    2.896866    0.632400  00:19     
96        0.025247    2.977653    0.628600  00:19     
97        0.023868    2.868821    0.628900  00:19     
98        0.023024    2.951335    0.628600  00:19     
99        0.032695    3.102293    0.625400  00:18     
