epoch     train_loss  valid_loss  accuracy  time    
0         12.530942   #na#        00:02     
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
Min numerical gradient: 5.75E-04
Min loss divided by 10: 1.91E-03
epoch     train_loss  valid_loss  accuracy  time    
/home/gobind/miniconda3/lib/python3.7/site-packages/fastai/sixel.py:16: UserWarning: You could see this plot with `libsixel`. See https://github.com/saitoha/libsixel warn("You could see this plot with `libsixel`. See https://github.com/saitoha/libsixel")
0         1.418239    1.422548    0.487600  00:09     
1         1.200594    1.266090    0.546900  00:09     
2         1.065323    1.191333    0.580600  00:09     
3         0.897012    1.156486    0.601300  00:09     
4         0.770266    1.235146    0.584100  00:09     
5         0.639342    1.228111    0.618000  00:10     
6         0.505425    1.334280    0.601800  00:10     
7         0.414319    1.407658    0.607400  00:10     
8         0.331822    1.558943    0.611400  00:09     
9         0.278492    1.661044    0.605500  00:10     
10        0.233081    1.760930    0.608300  00:18     
11        0.207214    1.816893    0.606900  00:18     
12        0.178869    1.903760    0.605100  00:17     
13        0.178543    1.929165    0.608300  00:17     
14        0.167635    1.860085    0.614700  00:17     
15        0.155169    1.992003    0.605800  00:18     
16        0.134179    2.016663    0.616000  00:18     
17        0.126436    2.035922    0.612100  00:18     
18        0.124707    2.145158    0.610500  00:18     
19        0.110467    2.208972    0.616600  00:18     
20        0.116024    2.125818    0.612700  00:18     
21        0.084583    2.136491    0.626300  00:18     
22        0.098815    2.171560    0.612700  00:18     
23        0.090669    2.383267    0.608700  00:19     
24        0.088166    2.197994    0.614200  00:19     
25        0.087001    2.230944    0.617600  00:18     
26        0.084250    2.302813    0.618800  00:18     
27        0.082552    2.255725    0.618500  00:18     
28        0.077169    2.278278    0.618900  00:18     
29        0.083291    2.325549    0.615600  00:18     
30        0.071776    2.269591    0.627700  00:18     
31        0.072284    2.308291    0.628200  00:18     
32        0.074909    2.305020    0.623800  00:18     
33        0.065596    2.372843    0.611000  00:18     
34        0.070673    2.361955    0.618800  00:18     
35        0.067835    2.372968    0.617400  00:18     
36        0.075055    2.518821    0.616500  00:18     
37        0.055362    2.494457    0.623700  00:18     
38        0.052951    2.450686    0.620200  00:18     
39        0.060706    2.426247    0.618300  00:18     
40        0.060523    2.505345    0.613200  00:18     
41        0.062815    2.450993    0.621100  00:18     
42        0.059767    2.473194    0.615800  00:18     
43        0.056915    2.415028    0.626800  00:18     
44        0.053564    2.503389    0.625300  00:18     
45        0.057673    2.587009    0.615500  00:18     
46        0.050688    2.581094    0.621000  00:18     
47        0.052546    2.603930    0.622300  00:18     
48        0.043541    2.596452    0.626700  00:18     
49        0.053056    2.512812    0.627300  00:18     
50        0.040957    2.618620    0.628900  00:18     
51        0.050232    2.587739    0.623600  00:18     
52        0.048311    2.594223    0.627900  00:18     
53        0.050223    2.610435    0.620800  00:18     
54        0.043486    2.651834    0.615400  00:18     
55        0.045834    2.532542    0.632300  00:18     
56        0.046756    2.671919    0.625200  00:18     
57        0.043393    2.692438    0.623200  00:18     
58        0.037371    2.694792    0.626300  00:18     
59        0.040384    2.732862    0.619900  00:18     
60        0.039941    2.673844    0.629500  00:18     
61        0.042881    2.715451    0.625500  00:18     
62        0.034039    2.641558    0.628500  00:18     
63        0.036801    2.693187    0.626500  00:18     
64        0.037566    2.755503    0.624500  00:18     
65        0.030690    2.740810    0.631400  00:18     
66        0.033816    2.883468    0.622600  00:18     
67        0.029078    2.767305    0.626100  00:18     
68        0.029390    2.666706    0.630300  00:19     
69        0.026785    2.725664    0.628600  00:19     
70        0.029889    2.761904    0.623100  00:18     
71        0.037655    2.851882    0.621000  00:18     
72        0.030481    2.716602    0.626400  00:18     
73        0.036133    2.839937    0.632700  00:18     
74        0.032170    2.711575    0.629800  00:18     
75        0.031326    2.774427    0.625700  00:18     
76        0.026508    2.829550    0.635700  00:18     
77        0.027514    2.814305    0.632800  00:18     
78        0.030992    2.837604    0.627900  00:18     
79        0.031171    2.837689    0.631800  00:18     
80        0.029802    2.772048    0.626800  00:18     
81        0.031934    2.868280    0.624400  00:18     
82        0.030014    2.928900    0.624400  00:18     
83        0.025842    2.818981    0.629200  00:18     
84        0.028821    2.879384    0.627600  00:18     
85        0.026098    2.817262    0.635000  00:18     
86        0.028223    2.823191    0.629200  00:18     
87        0.029042    2.902881    0.629300  00:18     
88        0.030508    2.856621    0.631900  00:18     
89        0.027141    2.785024    0.625300  00:18     
90        0.022220    2.830155    0.633500  00:18     
91        0.029780    2.882494    0.625400  00:18     
92        0.023594    2.855346    0.633100  00:18     
93        0.023257    2.910795    0.631900  00:18     
94        0.030100    3.010518    0.631600  00:18     
95        0.027327    2.852116    0.626000  00:18     
96        0.026533    2.839677    0.629900  00:18     
97        0.023613    2.878065    0.630400  00:18     
98        0.030464    2.897867    0.625200  00:18     
99        0.024770    2.899242    0.632400  00:18     
