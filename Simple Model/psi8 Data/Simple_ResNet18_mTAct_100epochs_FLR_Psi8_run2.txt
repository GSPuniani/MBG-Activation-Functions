epoch     train_loss  valid_loss  accuracy  time    
0         11.681287   #na#        00:05     
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
Min numerical gradient: 8.32E-04
Min loss divided by 10: 1.91E-03
epoch     train_loss  valid_loss  accuracy  time    
/home/gobind/miniconda3/lib/python3.7/site-packages/fastai/sixel.py:16: UserWarning: You could see this plot with `libsixel`. See https://github.com/saitoha/libsixel warn("You could see this plot with `libsixel`. See https://github.com/saitoha/libsixel")
0         1.392998    1.376424    0.501600  00:17     
1         1.195884    1.239247    0.563800  00:17     
2         1.039705    1.136646    0.603100  00:17     
3         0.913017    1.058160    0.633500  00:17     
4         0.791204    1.075413    0.631000  00:17     
5         0.657397    1.131855    0.628600  00:17     
6         0.540238    1.177819    0.628900  00:17     
7         0.431605    1.307684    0.641200  00:15     
8         0.362767    1.341666    0.645600  00:16     
9         0.302398    1.486162    0.627500  00:17     
10        0.253623    1.625679    0.632000  00:17     
11        0.215668    1.622062    0.639700  00:17     
12        0.191944    1.763305    0.634600  00:17     
13        0.181256    1.791444    0.636700  00:17     
14        0.155285    1.846922    0.628400  00:17     
15        0.142569    1.901239    0.639500  00:17     
16        0.139092    1.960719    0.631700  00:17     
17        0.125072    2.015400    0.637500  00:17     
18        0.115640    1.992722    0.636600  00:17     
19        0.122012    1.995141    0.631500  00:17     
20        0.110765    1.978903    0.635300  00:17     
21        0.105576    2.075885    0.640600  00:17     
22        0.096230    2.064658    0.643000  00:17     
23        0.086852    2.154723    0.630300  00:17     
24        0.080343    2.113144    0.638700  00:17     
25        0.082604    2.061728    0.636300  00:17     
26        0.081060    2.162523    0.646000  00:17     
27        0.074030    2.187386    0.635700  00:17     
28        0.068634    2.234221    0.629700  00:17     
29        0.071220    2.212101    0.646200  00:17     
30        0.066274    2.220965    0.646000  00:17     
31        0.069668    2.216064    0.642200  00:17     
32        0.071604    2.350180    0.628200  00:17     
33        0.057731    2.256836    0.643600  00:17     
34        0.062026    2.338925    0.642600  00:17     
35        0.061315    2.299901    0.636600  00:17     
36        0.057719    2.347140    0.645600  00:17     
37        0.055130    2.321031    0.639100  00:17     
38        0.061322    2.308779    0.640400  00:17     
39        0.057192    2.364330    0.644400  00:17     
40        0.051930    2.395855    0.635700  00:17     
41        0.055122    2.292815    0.644900  00:17     
42        0.056962    2.321314    0.646400  00:17     
43        0.057721    2.397309    0.642100  00:17     
44        0.049027    2.387392    0.646900  00:18     
45        0.052695    2.443739    0.640400  00:17     
46        0.045876    2.407033    0.640700  00:17     
47        0.046757    2.389640    0.649700  00:17     
48        0.043976    2.445707    0.639700  00:17     
49        0.044152    2.439554    0.649400  00:17     
50        0.046065    2.536978    0.638100  00:17     
51        0.040483    2.523824    0.650800  00:17     
52        0.044550    2.430221    0.647400  00:17     
53        0.039041    2.380271    0.652000  00:17     
54        0.041853    2.461599    0.645800  00:17     
55        0.039817    2.504949    0.641900  00:17     
56        0.040621    2.482979    0.640000  00:18     
57        0.039467    2.600088    0.641800  00:17     
58        0.039556    2.613295    0.639700  00:17     
59        0.041224    2.538765    0.644200  00:17     
60        0.042984    2.540062    0.646100  00:17     
61        0.037068    2.604815    0.643000  00:17     
62        0.037988    2.578810    0.645500  00:17     
63        0.032683    2.601046    0.642900  00:17     
64        0.035065    2.540059    0.652000  00:17     
65        0.037824    2.561290    0.648200  00:17     
66        0.032543    2.533605    0.648600  00:17     
67        0.030160    2.555927    0.647000  00:17     
68        0.033026    2.633260    0.652600  00:17     
69        0.028875    2.541787    0.645400  00:17     
70        0.034004    2.550050    0.651500  00:17     
71        0.034013    2.593678    0.643300  00:17     
72        0.030259    2.601572    0.648100  00:17     
73        0.036424    2.670996    0.645000  00:17     
74        0.028016    2.568139    0.645600  00:17     
75        0.029522    2.566786    0.651700  00:17     
76        0.034517    2.663338    0.645100  00:17     
77        0.033443    2.608085    0.646700  00:17     
78        0.028567    2.778781    0.643700  00:17     
79        0.028996    2.730970    0.643800  00:17     
80        0.023622    2.719885    0.647000  00:17     
81        0.032257    2.664252    0.652200  00:17     
82        0.028509    2.705809    0.649100  00:17     
83        0.026468    2.802965    0.647500  00:17     
84        0.025767    2.662597    0.658500  00:17     
85        0.021959    2.649780    0.651500  00:17     
86        0.031088    2.808200    0.652500  00:17     
87        0.026589    2.686884    0.639700  00:17     
88        0.022760    2.778165    0.643800  00:17     
89        0.029905    2.820104    0.648200  00:17     
90        0.026827    2.758582    0.642400  00:17     
91        0.028257    2.741343    0.648200  00:17     
92        0.031883    2.837452    0.644500  00:18     
93        0.025555    2.793586    0.644500  00:17     
94        0.023681    2.684841    0.654400  00:17     
95        0.025532    2.810043    0.650800  00:17     
96        0.022793    2.752870    0.644800  00:17     
97        0.024633    2.828742    0.647600  00:18     
98        0.027832    2.706628    0.642700  00:17     
99        0.022521    2.825402    0.642500  00:18     
