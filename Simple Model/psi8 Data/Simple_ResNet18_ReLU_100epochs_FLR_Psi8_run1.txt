epoch     train_loss  valid_loss  accuracy  time    
0         12.206941   #na#        00:04     
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
Min numerical gradient: 3.98E-04
Min loss divided by 10: 1.58E-03
epoch     train_loss  valid_loss  accuracy  time    
/home/gobind/miniconda3/lib/python3.7/site-packages/fastai/sixel.py:16: UserWarning: You could see this plot with `libsixel`. See https://github.com/saitoha/libsixel warn("You could see this plot with `libsixel`. See https://github.com/saitoha/libsixel")
0         1.464553    1.444328    0.477900  00:19     
1         1.245893    1.313180    0.537400  00:19     
2         1.075669    1.218274    0.566000  00:19     
3         0.898457    1.244272    0.576200  00:19     
4         0.752237    1.314129    0.579800  00:19     
5         0.591858    1.445524    0.569700  00:19     
6         0.454399    1.476343    0.586100  00:19     
7         0.376085    1.608547    0.574300  00:18     
8         0.297032    1.717376    0.581200  00:15     
9         0.246048    1.815883    0.584000  00:19     
10        0.225791    1.847544    0.583200  00:19     
11        0.193998    1.955437    0.582900  00:19     
12        0.179895    2.063475    0.590100  00:19     
13        0.184340    2.144743    0.574600  00:19     
14        0.156053    2.163977    0.584900  00:19     
15        0.147046    2.112325    0.587500  00:19     
16        0.127943    2.206111    0.581100  00:19     
17        0.124142    2.138751    0.587800  00:16     
18        0.130913    2.222353    0.586100  00:17     
19        0.119422    2.296196    0.578200  00:19     
20        0.122581    2.266596    0.584000  00:19     
21        0.108716    2.256724    0.594200  00:19     
22        0.105698    2.312439    0.597800  00:19     
23        0.100265    2.368005    0.592600  00:19     
24        0.089014    2.426131    0.579900  00:19     
25        0.101969    2.373424    0.591100  00:19     
26        0.095638    2.404907    0.592200  00:19     
27        0.079989    2.370118    0.599300  00:14     
28        0.075558    2.423807    0.596000  00:19     
29        0.076565    2.475441    0.594000  00:19     
30        0.073450    2.486897    0.595000  00:19     
31        0.068297    2.584866    0.591200  00:19     
32        0.075877    2.497334    0.602400  00:19     
33        0.075236    2.534537    0.599200  00:19     
34        0.065088    2.594618    0.597400  00:19     
35        0.063446    2.583852    0.597600  00:19     
36        0.066809    2.505329    0.598300  00:17     
37        0.061852    2.617084    0.594300  00:16     
38        0.066693    2.588315    0.599400  00:19     
39        0.063836    2.505883    0.598000  00:19     
40        0.057281    2.551638    0.600200  00:19     
41        0.054386    2.612248    0.598200  00:19     
42        0.058266    2.582682    0.598900  00:19     
43        0.050258    2.627147    0.604300  00:19     
44        0.054944    2.682037    0.592200  00:19     
45        0.050072    2.613400    0.602900  00:19     
46        0.048800    2.674500    0.604000  00:14     
47        0.053476    2.638469    0.599100  00:18     
48        0.058217    2.757214    0.597500  00:19     
49        0.051541    2.724519    0.598900  00:19     
50        0.047111    2.662905    0.601600  00:19     
51        0.042804    2.628561    0.602000  00:19     
52        0.041925    2.700387    0.600400  00:19     
53        0.049216    2.715476    0.599200  00:19     
54        0.043419    2.673277    0.605000  00:19     
55        0.043318    2.615343    0.610200  00:18     
56        0.044716    2.826563    0.605400  00:15     
57        0.036375    2.802632    0.605300  00:19     
58        0.039823    2.793374    0.601100  00:19     
59        0.044030    2.719128    0.604800  00:19     
60        0.041126    2.730792    0.608200  00:19     
61        0.045306    2.787238    0.606400  00:19     
62        0.042864    2.756349    0.603100  00:19     
63        0.035754    2.837008    0.604900  00:19     
64        0.036854    2.833119    0.607900  00:19     
65        0.044872    2.764184    0.606500  00:16     
66        0.036819    2.846849    0.606900  00:17     
67        0.036858    2.863203    0.609800  00:19     
68        0.038865    2.847464    0.602500  00:19     
69        0.035406    2.816941    0.605400  00:19     
70        0.033503    2.793210    0.602600  00:19     
71        0.034789    2.847324    0.609700  00:19     
72        0.035524    2.819730    0.602200  00:19     
73        0.031194    2.832023    0.603200  00:19     
74        0.032219    2.881683    0.608200  00:19     
75        0.032350    2.906959    0.604600  00:14     
76        0.034919    2.821401    0.613000  00:19     
77        0.034389    2.912361    0.601100  00:19     
78        0.032515    2.976487    0.604100  00:19     
79        0.033827    3.011208    0.606300  00:19     
80        0.030389    2.975009    0.606100  00:19     
81        0.036182    2.917196    0.604300  00:19     
82        0.036354    2.970663    0.608900  00:19     
83        0.031033    2.940515    0.607200  00:19     
84        0.031811    2.906109    0.612700  00:17     
85        0.027862    2.958483    0.612200  00:17     
86        0.028286    2.915750    0.608400  00:19     
87        0.030567    2.940223    0.610700  00:19     
88        0.031733    2.937279    0.607300  00:19     
89        0.033533    2.920205    0.612200  00:19     
90        0.036611    2.908853    0.613200  00:19     
91        0.027186    2.896871    0.614900  00:19     
92        0.029283    3.022585    0.612900  00:19     
93        0.028944    2.945111    0.610300  00:19     
94        0.031965    2.976780    0.603900  00:14     
95        0.024191    3.025512    0.610300  00:19     
96        0.032532    2.992752    0.607800  00:19     
97        0.028001    3.086605    0.610400  00:19     
98        0.030350    2.973770    0.613000  00:19     
99        0.022918    3.000613    0.610200  00:19     
