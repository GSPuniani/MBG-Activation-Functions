epoch     train_loss  valid_loss  accuracy  time    
0         14.178864   #na#        00:05     
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
Min numerical gradient: 1.20E-03
Min loss divided by 10: 1.58E-03
epoch     train_loss  valid_loss  accuracy  time    
/home/gobind/miniconda3/lib/python3.7/site-packages/fastai/sixel.py:16: UserWarning: You could see this plot with `libsixel`. See https://github.com/saitoha/libsixel warn("You could see this plot with `libsixel`. See https://github.com/saitoha/libsixel")
0         1.392026    1.425905    0.491700  00:18     
1         1.193731    1.260143    0.544800  00:17     
2         1.041617    1.158311    0.590000  00:17     
3         0.930958    1.046270    0.635600  00:17     
4         0.793113    1.082969    0.630300  00:17     
5         0.680081    1.211873    0.618400  00:17     
6         0.572575    1.145737    0.639700  00:17     
7         0.466552    1.201388    0.646600  00:11     
8         0.378207    1.328645    0.643200  00:18     
9         0.312647    1.474692    0.636500  00:18     
10        0.251381    1.495431    0.642300  00:18     
11        0.229615    1.635263    0.636000  00:18     
12        0.186514    1.685094    0.634000  00:18     
13        0.170874    1.795435    0.636900  00:18     
14        0.166965    1.792737    0.640200  00:18     
15        0.141373    1.801837    0.636700  00:18     
16        0.126626    1.904822    0.640400  00:18     
17        0.122055    2.013281    0.639600  00:18     
18        0.116152    1.885881    0.640200  00:17     
19        0.112255    2.006026    0.640900  00:17     
20        0.106036    2.031026    0.637700  00:18     
21        0.105392    2.056177    0.639800  00:17     
22        0.103013    2.039167    0.640400  00:17     
23        0.085059    2.213371    0.632700  00:19     
24        0.090795    1.997633    0.649600  00:28     
25        0.080891    2.098776    0.639100  00:28     
26        0.075628    2.187604    0.641400  00:28     
27        0.086159    2.079237    0.640500  00:28     
28        0.068503    2.215535    0.633500  00:28     
29        0.070982    2.311189    0.634800  00:28     
30        0.077726    2.171305    0.646400  00:28     
31        0.058681    2.170168    0.645800  00:28     
32        0.055997    2.303638    0.641200  00:22     
33        0.057608    2.245026    0.643200  00:17     
34        0.062274    2.314709    0.640300  00:17     
35        0.054412    2.343966    0.642400  00:17     
36        0.054889    2.290241    0.645700  00:17     
37        0.064441    2.242621    0.639900  00:17     
38        0.064776    2.341413    0.639400  00:17     
39        0.054518    2.314067    0.644000  00:17     
40        0.057685    2.432640    0.640800  00:17     
41        0.059072    2.384717    0.640200  00:17     
42        0.049769    2.461688    0.635100  00:17     
43        0.051314    2.376125    0.644000  00:17     
44        0.052956    2.443943    0.642500  00:17     
45        0.047150    2.439471    0.644600  00:17     
46        0.043995    2.453951    0.646500  00:20     
47        0.043857    2.421353    0.646100  00:28     
48        0.039271    2.483159    0.645500  00:28     
49        0.039322    2.486581    0.646400  00:28     
50        0.046775    2.427655    0.648600  00:28     
51        0.052598    2.304551    0.649800  00:28     
52        0.047644    2.473687    0.647300  00:28     
53        0.045517    2.435777    0.642100  00:28     
54        0.036764    2.491055    0.646300  00:28     
55        0.058697    2.507453    0.637600  00:22     
56        0.051559    2.348181    0.645800  00:17     
57        0.036768    2.546714    0.640100  00:17     
58        0.035402    2.453684    0.643300  00:17     
59        0.031928    2.512095    0.643900  00:17     
60        0.038793    2.526651    0.643700  00:17     
61        0.040212    2.455920    0.644400  00:17     
62        0.032298    2.553459    0.657500  00:17     
63        0.039206    2.567243    0.637500  00:17     
64        0.035650    2.569460    0.645300  00:17     
65        0.033534    2.536504    0.648600  00:17     
66        0.038557    2.570445    0.639600  00:17     
67        0.033402    2.546693    0.639700  00:17     
68        0.042068    2.537427    0.646300  00:17     
69        0.030157    2.574467    0.640500  00:17     
70        0.027659    2.590607    0.643300  00:17     
71        0.028829    2.541146    0.647100  00:18     
72        0.036229    2.654823    0.644700  00:18     
73        0.030224    2.584708    0.647400  00:17     
74        0.030732    2.682079    0.645400  00:17     
75        0.031579    2.551483    0.645200  00:17     
76        0.031200    2.599226    0.644500  00:17     
77        0.035122    2.661098    0.642200  00:17     
78        0.038286    2.580048    0.645900  00:17     
79        0.037242    2.680104    0.645000  00:18     
80        0.029328    2.622466    0.643600  00:17     
81        0.031134    2.660569    0.643000  00:18     
82        0.025360    2.604438    0.644300  00:17     
83        0.025831    2.681154    0.639200  00:17     
84        0.030671    2.544047    0.647300  00:17     
85        0.029471    2.702287    0.641200  00:18     
86        0.028171    2.484366    0.644200  00:17     
87        0.023179    2.646399    0.652300  00:17     
88        0.026921    2.684207    0.647300  00:17     
89        0.025626    2.670861    0.650800  00:17     
90        0.027919    2.663924    0.646500  00:17     
91        0.029994    2.606470    0.651900  00:17     
92        0.028665    2.632025    0.643900  00:17     
93        0.027987    2.624873    0.653400  00:20     
94        0.026654    2.554179    0.643100  00:28     
95        0.027858    2.705385    0.641100  00:28     
96        0.025030    2.720548    0.644200  00:28     
97        0.028209    2.728196    0.646500  00:28     
98        0.025468    2.613161    0.647100  00:28     
99        0.023737    2.830457    0.643700  00:28     
