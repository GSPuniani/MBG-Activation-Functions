epoch     train_loss  valid_loss  accuracy  time    
0         19.597708   #na#        00:03     
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
Min numerical gradient: 6.92E-04
Min loss divided by 10: 1.91E-03
epoch     train_loss  valid_loss  accuracy  time    
/home/gobind/miniconda3/lib/python3.7/site-packages/fastai/sixel.py:16: UserWarning: You could see this plot with `libsixel`. See https://github.com/saitoha/libsixel warn("You could see this plot with `libsixel`. See https://github.com/saitoha/libsixel")
0         1.410036    1.640036    0.431600  00:18     
1         1.190460    1.239369    0.559000  00:19     
2         1.035247    1.169845    0.587900  00:19     
3         0.908831    1.157587    0.597400  00:19     
4         0.788926    1.081658    0.624600  00:18     
5         0.633936    1.217277    0.616100  00:18     
6         0.509109    1.308778    0.624100  00:18     
7         0.416836    1.451065    0.614500  00:18     
8         0.343086    1.515182    0.616800  00:18     
9         0.290913    1.710121    0.611000  00:11     
10        0.246361    1.707990    0.620600  00:18     
11        0.215941    1.779835    0.615400  00:18     
12        0.192995    1.867077    0.616300  00:18     
13        0.170429    1.876176    0.623000  00:18     
14        0.171739    2.030357    0.607800  00:18     
15        0.132997    2.055596    0.614800  00:18     
16        0.136898    2.009561    0.622600  00:18     
17        0.124686    1.998845    0.625100  00:18     
18        0.121306    2.065446    0.624400  00:18     
19        0.106534    2.088452    0.625900  00:18     
20        0.113430    2.072912    0.629000  00:18     
21        0.098772    2.183470    0.624300  00:18     
22        0.102108    2.115815    0.631300  00:18     
23        0.099851    2.203214    0.625500  00:18     
24        0.097498    2.181162    0.626300  00:17     
25        0.088868    2.184910    0.624500  00:17     
26        0.077747    2.246133    0.632300  00:17     
27        0.080082    2.256079    0.627200  00:17     
28        0.082891    2.298132    0.623900  00:17     
29        0.082563    2.378333    0.630400  00:17     
30        0.077700    2.326538    0.628500  00:17     
31        0.072012    2.320224    0.627100  00:17     
32        0.073145    2.380979    0.621000  00:17     
33        0.071049    2.370929    0.627800  00:17     
34        0.057461    2.390547    0.630000  00:17     
35        0.062046    2.461885    0.615900  00:17     
36        0.069614    2.380944    0.635800  00:17     
37        0.058985    2.529575    0.624000  00:17     
38        0.059583    2.399415    0.622000  00:17     
39        0.055529    2.370222    0.627600  00:17     
40        0.052381    2.485527    0.628100  00:17     
41        0.058586    2.455115    0.623600  00:17     
42        0.054850    2.466014    0.630200  00:17     
43        0.049488    2.508422    0.628200  00:17     
44        0.053378    2.540833    0.620100  00:17     
45        0.048029    2.558537    0.629600  00:17     
46        0.046264    2.561016    0.630900  00:17     
47        0.041452    2.588908    0.637200  00:17     
48        0.039199    2.631952    0.634600  00:17     
49        0.046064    2.717064    0.627100  00:17     
50        0.044523    2.584553    0.630900  00:17     
51        0.044346    2.605821    0.627300  00:17     
52        0.039879    2.625499    0.634100  00:17     
53        0.039188    2.554223    0.631200  00:17     
54        0.047453    2.642524    0.627200  00:17     
55        0.039603    2.673040    0.630000  00:17     
56        0.039856    2.561986    0.638000  00:17     
57        0.042014    2.638569    0.632600  00:17     
58        0.039682    2.690284    0.631200  00:17     
59        0.040635    2.784831    0.628100  00:17     
60        0.037074    2.696577    0.632400  00:17     
61        0.034839    2.806300    0.632700  00:17     
62        0.035805    2.773760    0.632400  00:17     
63        0.033497    2.641351    0.631400  00:17     
64        0.037688    2.715135    0.638700  00:17     
65        0.036057    2.843048    0.633000  00:17     
66        0.028911    2.731914    0.633300  00:17     
67        0.036160    2.664164    0.638400  00:17     
68        0.031414    2.621634    0.640600  00:17     
69        0.030866    2.677548    0.635600  00:17     
70        0.026977    2.699551    0.634200  00:17     
71        0.030485    2.740273    0.639800  00:17     
72        0.032730    2.744818    0.637900  00:17     
73        0.030551    2.691544    0.631700  00:17     
74        0.036907    2.798209    0.635200  00:17     
75        0.033123    2.638762    0.632800  00:17     
76        0.023722    2.772680    0.641900  00:17     
77        0.034758    2.776111    0.636700  00:17     
78        0.036989    2.771177    0.635400  00:17     
79        0.025327    2.728670    0.638000  00:17     
80        0.028046    2.808631    0.636300  00:17     
81        0.034793    2.978375    0.619100  00:17     
82        0.023588    2.789431    0.641200  00:17     
83        0.029416    2.839166    0.635000  00:17     
84        0.031909    2.816484    0.630900  00:17     
85        0.025670    2.772015    0.639900  00:17     
86        0.033099    2.782519    0.634300  00:17     
87        0.030501    2.748341    0.639600  00:17     
88        0.022701    2.828633    0.640400  00:17     
89        0.026994    2.841163    0.636200  00:17     
90        0.019413    2.792358    0.645100  00:17     
91        0.028603    2.806580    0.637200  00:17     
92        0.026824    2.862941    0.640700  00:18     
93        0.026437    2.870238    0.636700  00:18     
94        0.025008    2.750837    0.639700  00:18     
95        0.022228    2.855551    0.641600  00:18     
96        0.024460    2.887228    0.644900  00:18     
97        0.025256    2.852954    0.642200  00:18     
98        0.020759    2.824006    0.635600  00:18     
99        0.024642    2.815619    0.640900  00:18     
