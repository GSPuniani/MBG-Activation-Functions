epoch     train_loss  valid_loss  accuracy  time    
0         10.440291   #na#        00:05     
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
Min numerical gradient: 6.92E-04
Min loss divided by 10: 3.63E-04
epoch     train_loss  valid_loss  accuracy  time    
/home/gobind/miniconda3/lib/python3.7/site-packages/fastai/sixel.py:16: UserWarning: You could see this plot with `libsixel`. See https://github.com/saitoha/libsixel warn("You could see this plot with `libsixel`. See https://github.com/saitoha/libsixel")
0         1.402037    1.370135    0.506500  00:18     
1         1.203870    1.293576    0.543300  00:18     
2         1.039826    1.133082    0.599800  00:17     
3         0.921935    1.146541    0.609800  00:17     
4         0.784879    1.158085    0.611600  00:17     
5         0.667141    1.158789    0.628400  00:17     
6         0.523214    1.246442    0.621500  00:17     
7         0.431725    1.398856    0.622400  00:12     
8         0.349188    1.529104    0.612700  00:18     
9         0.289331    1.589000    0.620900  00:17     
10        0.246273    1.620449    0.626400  00:17     
11        0.203776    1.789425    0.614500  00:17     
12        0.203515    1.826104    0.621400  00:17     
13        0.176896    1.869052    0.620500  00:17     
14        0.168440    1.950050    0.625100  00:17     
15        0.152055    2.068415    0.614600  00:17     
16        0.135286    2.059700    0.618500  00:17     
17        0.133269    2.100451    0.613000  00:17     
18        0.124515    2.060360    0.621700  00:17     
19        0.111328    2.103523    0.623500  00:17     
20        0.105448    2.139721    0.630000  00:17     
21        0.104075    2.193186    0.627300  00:17     
22        0.096887    2.194698    0.620600  00:17     
23        0.093990    2.124227    0.624000  00:17     
24        0.093612    2.286810    0.614600  00:17     
25        0.082890    2.235578    0.622100  00:17     
26        0.080278    2.318596    0.627000  00:17     
27        0.077834    2.326738    0.625500  00:17     
28        0.080785    2.195601    0.619400  00:17     
29        0.077300    2.239245    0.629500  00:17     
30        0.069782    2.345113    0.627900  00:17     
31        0.069080    2.464117    0.621300  00:17     
32        0.073038    2.354899    0.632000  00:17     
33        0.066008    2.296008    0.636800  00:17     
34        0.065054    2.351249    0.629100  00:17     
35        0.066195    2.390370    0.624400  00:17     
36        0.063960    2.400577    0.632100  00:17     
37        0.055558    2.447572    0.628100  00:17     
38        0.053036    2.529637    0.630200  00:17     
39        0.054959    2.484716    0.633800  00:17     
40        0.049517    2.430135    0.633600  00:17     
41        0.057444    2.505865    0.630700  00:17     
42        0.047810    2.479035    0.636600  00:17     
43        0.062938    2.494408    0.632600  00:17     
44        0.056468    2.487907    0.631700  00:17     
45        0.038184    2.474150    0.637700  00:17     
46        0.047374    2.482883    0.639800  00:17     
47        0.053652    2.490771    0.632200  00:17     
48        0.045263    2.545757    0.632600  00:17     
49        0.048174    2.518445    0.630900  00:17     
50        0.038846    2.568747    0.630100  00:17     
51        0.050295    2.632515    0.633600  00:17     
52        0.037457    2.617977    0.637500  00:17     
53        0.043505    2.622689    0.634500  00:17     
54        0.039916    2.668472    0.629500  00:17     
55        0.039396    2.658252    0.629300  00:17     
56        0.041206    2.624426    0.630400  00:17     
57        0.042997    2.602148    0.634900  00:17     
58        0.039012    2.652833    0.639700  00:17     
59        0.039425    2.688188    0.639600  00:17     
60        0.043069    2.603345    0.638000  00:17     
61        0.038862    2.628521    0.636900  00:17     
62        0.037333    2.604825    0.639200  00:17     
63        0.037939    2.715955    0.629600  00:17     
64        0.037290    2.728176    0.631100  00:17     
65        0.030647    2.644147    0.640100  00:17     
66        0.032295    2.776928    0.637700  00:17     
67        0.032860    2.822993    0.634200  00:17     
68        0.034443    2.768229    0.633700  00:17     
69        0.038638    2.706184    0.640600  00:17     
70        0.029291    2.695628    0.631500  00:17     
71        0.030844    2.794375    0.633300  00:17     
72        0.028625    2.719766    0.633600  00:17     
73        0.031356    2.705898    0.637000  00:17     
74        0.038439    2.799296    0.631000  00:17     
75        0.029425    2.842528    0.632400  00:17     
76        0.028874    2.686327    0.639900  00:17     
77        0.029812    2.780880    0.633500  00:17     
78        0.027612    2.774045    0.637400  00:17     
79        0.031694    2.786995    0.634300  00:17     
80        0.033899    2.766301    0.630100  00:17     
81        0.027994    2.751266    0.636700  00:17     
82        0.027197    2.875585    0.631900  00:17     
83        0.024929    2.933427    0.634300  00:17     
84        0.022329    2.813412    0.638300  00:17     
85        0.028336    2.860056    0.631800  00:18     
86        0.024775    2.824791    0.641100  00:18     
87        0.026122    2.882988    0.636800  00:18     
88        0.024827    2.911244    0.629700  00:18     
89        0.024860    2.948276    0.639100  00:18     
90        0.030768    2.844437    0.633800  00:18     
91        0.025828    2.823426    0.643900  00:18     
92        0.026417    2.774428    0.640400  00:18     
93        0.026992    2.819720    0.638400  00:18     
94        0.025597    2.813793    0.639600  00:18     
95        0.022581    2.858723    0.637400  00:18     
96        0.020743    2.806716    0.643000  00:18     
97        0.018149    2.830838    0.643400  00:18     
98        0.027146    2.926921    0.638400  00:18     
99        0.023230    2.777849    0.639000  00:18     
