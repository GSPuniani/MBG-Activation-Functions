epoch     train_loss  valid_loss  accuracy  time    
0         11.785251   #na#        00:05     
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
Min numerical gradient: 8.32E-04
Min loss divided by 10: 1.58E-03
epoch     train_loss  valid_loss  accuracy  time    
/home/gobind/miniconda3/lib/python3.7/site-packages/fastai/sixel.py:16: UserWarning: You could see this plot with `libsixel`. See https://github.com/saitoha/libsixel warn("You could see this plot with `libsixel`. See https://github.com/saitoha/libsixel")
0         1.383695    1.352147    0.511000  00:18     
1         1.183505    1.232497    0.563400  00:18     
2         1.027906    1.196841    0.582100  00:18     
3         0.898394    1.078280    0.629300  00:18     
4         0.784448    1.096194    0.630200  00:18     
5         0.670900    1.124285    0.631700  00:18     
6         0.537463    1.183517    0.628700  00:18     
7         0.450295    1.303437    0.629200  00:11     
8         0.378313    1.412410    0.623300  00:18     
9         0.294394    1.544164    0.621600  00:18     
10        0.245888    1.727417    0.623100  00:18     
11        0.226077    1.744973    0.624500  00:18     
12        0.200783    1.828595    0.621000  00:18     
13        0.181398    1.762352    0.635500  00:18     
14        0.160589    1.846410    0.621100  00:18     
15        0.155268    1.938095    0.617700  00:18     
16        0.123501    1.971772    0.629900  00:18     
17        0.112577    1.970399    0.627300  00:18     
18        0.101197    2.056259    0.631900  00:18     
19        0.115750    1.970385    0.633200  00:18     
20        0.106498    2.081004    0.634000  00:18     
21        0.113418    2.101841    0.628000  00:18     
22        0.095121    2.061414    0.633500  00:18     
23        0.087285    2.119432    0.636900  00:18     
24        0.092233    2.325850    0.623400  00:18     
25        0.092632    2.221404    0.636900  00:18     
26        0.078815    2.248482    0.635500  00:18     
27        0.079704    2.234127    0.633700  00:18     
28        0.079108    2.304835    0.634300  00:18     
29        0.079243    2.275293    0.626300  00:18     
30        0.070040    2.323371    0.622900  00:18     
31        0.070213    2.404420    0.628600  00:18     
32        0.067994    2.380733    0.629600  00:18     
33        0.069271    2.322668    0.631300  00:18     
34        0.064419    2.372528    0.629800  00:18     
35        0.065858    2.316488    0.633700  00:18     
36        0.056103    2.331500    0.639400  00:18     
37        0.055178    2.384908    0.636900  00:18     
38        0.053561    2.419980    0.634200  00:18     
39        0.055619    2.413372    0.638300  00:18     
40        0.057283    2.598996    0.627600  00:18     
41        0.057452    2.410539    0.633500  00:18     
42        0.050186    2.476910    0.633100  00:18     
43        0.061399    2.397919    0.638300  00:18     
44        0.048474    2.551575    0.636200  00:18     
45        0.048372    2.588539    0.637600  00:18     
46        0.050625    2.632061    0.625700  00:18     
47        0.046758    2.534716    0.634600  00:18     
48        0.039961    2.539344    0.634700  00:18     
49        0.049508    2.537952    0.631600  00:18     
50        0.041590    2.507642    0.643600  00:18     
51        0.041942    2.523502    0.634000  00:18     
52        0.044165    2.570241    0.631200  00:18     
53        0.037825    2.509776    0.643400  00:18     
54        0.041761    2.483461    0.639600  00:18     
55        0.035253    2.494882    0.646100  00:18     
56        0.039318    2.651964    0.634300  00:18     
57        0.042028    2.539127    0.639400  00:18     
58        0.031202    2.651365    0.633800  00:18     
59        0.040257    2.698673    0.636900  00:18     
60        0.033849    2.555591    0.638900  00:18     
61        0.037053    2.551464    0.638400  00:18     
62        0.042679    2.528531    0.638200  00:18     
63        0.034220    2.642401    0.638900  00:18     
64        0.034742    2.643247    0.639200  00:18     
65        0.035030    2.629050    0.634000  00:18     
66        0.032332    2.580273    0.641000  00:18     
67        0.036686    2.687909    0.638800  00:18     
68        0.031060    2.720164    0.640000  00:18     
69        0.034386    2.606727    0.642100  00:18     
70        0.037563    2.634502    0.640000  00:18     
71        0.030436    2.761634    0.633900  00:18     
72        0.032720    2.776774    0.633700  00:18     
73        0.029346    2.838597    0.631500  00:18     
74        0.031466    2.698991    0.638500  00:18     
75        0.034226    2.730654    0.632800  00:18     
76        0.024763    2.696124    0.637600  00:18     
77        0.030606    2.773039    0.635600  00:18     
78        0.027118    2.725177    0.632300  00:18     
79        0.024556    2.737585    0.636900  00:18     
80        0.030916    2.768358    0.638000  00:18     
81        0.031956    2.892559    0.635200  00:18     
82        0.024147    2.797397    0.639000  00:18     
83        0.029969    2.761186    0.633500  00:18     
84        0.025726    2.788470    0.635400  00:18     
85        0.030163    2.701971    0.637400  00:18     
86        0.032375    2.827204    0.634400  00:18     
87        0.028942    2.783749    0.643600  00:18     
88        0.030957    2.808084    0.637600  00:18     
89        0.025440    2.822228    0.638300  00:18     
90        0.024657    2.769963    0.643800  00:18     
91        0.032120    2.697151    0.640100  00:18     
92        0.029641    2.817465    0.636000  00:18     
93        0.027003    2.777761    0.637700  00:18     
94        0.028206    2.854297    0.632500  00:18     
95        0.026238    2.746366    0.640200  00:18     
96        0.024246    2.825606    0.636900  00:18     
97        0.029306    2.798224    0.635800  00:18     
98        0.024322    2.805316    0.644200  00:18     
99        0.017742    2.811340    0.644100  00:18     
