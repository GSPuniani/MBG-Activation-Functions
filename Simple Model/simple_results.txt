epoch     train_loss  valid_loss  accuracy  time    
0         27.110470   #na#        00:02     
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
Min numerical gradient: 1.20E-03
Min loss divided by 10: 1.10E-01
epoch     train_loss  valid_loss  accuracy  time    
/home/gobind/miniconda3/lib/python3.7/site-packages/fastai/sixel.py:16: UserWarning: You could see this plot with `libsixel`. See https://github.com/saitoha/libsixel
  warn("You could see this plot with `libsixel`. See https://github.com/saitoha/libsixel")
0         1.500706    1.424661    0.492900  00:09     
1         1.241198    1.274286    0.542100  00:09     
2         1.078623    1.243072    0.557200  00:09     
3         1.000635    1.154447    0.594400  00:09     
4         0.931333    1.060382    0.625600  00:09     
5         0.867655    1.089734    0.625900  00:09     
6         0.838121    0.985172    0.655000  00:10     
7         0.758136    1.001458    0.658100  00:09     
8         0.692397    0.928348    0.691100  00:09     
9         0.636203    0.985803    0.679300  00:10     
10        0.573609    0.943324    0.698700  00:10     
11        0.517073    0.887306    0.718300  00:09     
12        0.461165    0.919646    0.722100  00:10     
13        0.401897    0.935231    0.730000  00:09     
14        0.335245    0.951026    0.734300  00:09     
15        0.275017    1.022549    0.733900  00:09     
16        0.239224    1.039115    0.734400  00:10     
17        0.208098    1.000317    0.745500  00:09     
18        0.183921    1.214980    0.722600  00:10     
19        0.144120    1.199582    0.731600  00:09     
20        0.139269    1.197563    0.742100  00:10     
21        0.125022    1.256205    0.743100  00:09     
22        0.107216    1.235733    0.751700  00:10     
23        0.102032    1.347152    0.736900  00:10     
24        0.076513    1.269023    0.757200  00:10     
25        0.071694    1.309636    0.754100  00:10     
26        0.061053    1.325895    0.756400  00:10     
27        0.045600    1.324552    0.764200  00:10     
28        0.045360    1.437368    0.755100  00:10     
29        0.032352    1.386834    0.767400  00:10     
30        0.030303    1.443453    0.762000  00:10     
31        0.028826    1.483087    0.760100  00:10     
32        0.020540    1.490985    0.770500  00:10     
33        0.017517    1.554860    0.768400  00:10     
34        0.013740    1.532551    0.768400  00:09     
35        0.008762    1.593562    0.771600  00:10     
36        0.009962    1.598566    0.773700  00:10     
37        0.006778    1.601054    0.770800  00:10     
38        0.006056    1.658294    0.777800  00:10     
39        0.001953    1.675515    0.774900  00:10     
40        0.001175    1.701456    0.776500  00:10     
41        0.000954    1.720655    0.779600  00:10     
42        0.000745    1.738991    0.779300  00:10     
43        0.000272    1.763893    0.778400  00:10     
44        0.000089    1.763074    0.780100  00:10     
45        0.000078    1.792883    0.777800  00:10     
46        0.000143    1.802995    0.778100  00:09     
47        0.000057    1.797095    0.778000  00:10     
48        0.000027    1.808229    0.779300  00:10     
49        0.000066    1.796459    0.778600  00:09     
