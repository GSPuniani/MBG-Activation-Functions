/home/gobind/.fastai/data/cifar10
epoch     train_loss  valid_loss  accuracy  time    
0         21.937588   #na#        00:02     
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
Min numerical gradient: 3.02E-03
Min loss divided by 10: 6.31E-02
epoch     train_loss  valid_loss  accuracy  time    
0         1.516806    1.383449    0.507000  00:10     
1         1.263902    1.157483    0.583700  00:10     
2         1.132967    1.037052    0.627600  00:10     
3         1.042782    1.021620    0.641100  00:10     
4         0.953458    0.991622    0.651000  00:10     
5         0.883633    0.833392    0.706300  00:10     
6         0.837330    0.914865    0.682600  00:10     
7         0.792471    0.795130    0.720600  00:10     
8         0.737136    0.718652    0.747500  00:10     
9         0.693117    0.729930    0.747400  00:10     
10        0.638682    0.748252    0.746400  00:10     
11        0.614846    0.646195    0.778600  00:10     
12        0.576840    0.577146    0.802900  00:10     
13        0.574177    0.590686    0.795500  00:10     
14        0.510112    0.593031    0.798300  00:10     
15        0.483347    0.566690    0.809400  00:10     
16        0.471767    0.520594    0.824600  00:10     
17        0.430506    0.522054    0.825800  00:10     
18        0.424000    0.513170    0.826900  00:10     
19        0.393214    0.500542    0.830900  00:10     
20        0.368383    0.510672    0.837300  00:10     
21        0.357156    0.440577    0.849900  00:10     
22        0.344917    0.468117    0.847900  00:10     
23        0.317347    0.450214    0.850200  00:10     
24        0.305177    0.427933    0.863400  00:10     
25        0.291641    0.463433    0.850700  00:10     
26        0.274856    0.460345    0.852500  00:10     
27        0.248813    0.442393    0.861000  00:10     
28        0.246978    0.471243    0.856300  00:10     
29        0.233837    0.453583    0.863900  00:10     
30        0.206447    0.456411    0.864300  00:10     
31        0.182808    0.448120    0.868500  00:10     
32        0.173564    0.430428    0.872600  00:10     
33        0.167351    0.441218    0.869400  00:10     
34        0.151793    0.445765    0.871700  00:10     
35        0.137306    0.453545    0.874200  00:10     
36        0.120285    0.444352    0.874500  00:10     
37        0.119115    0.445397    0.878800  00:10     
38        0.100139    0.450674    0.878700  00:10     
39        0.094545    0.464030    0.882100  00:10     
40        0.081413    0.451928    0.883700  00:10     
41        0.079576    0.456073    0.881800  00:10     
42        0.072271    0.467091    0.885200  00:10     
43        0.064144    0.465168    0.885100  00:10     
44        0.058194    0.468609    0.886100  00:10     
45        0.058210    0.470586    0.884500  00:10     
46        0.056437    0.468608    0.884900  00:10     
47        0.053679    0.467895    0.886600  00:10     
48        0.052996    0.469607    0.885700  00:10     
49        0.050346    0.468687    0.884900  00:10     
